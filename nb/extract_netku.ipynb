{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f9f704-ae5a-4cf4-98d9-c3be31c74f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quert/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import json\n",
    "import contextlib, unicodedata, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c74818-7b68-4d1f-a66a-6f61a7473b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b1e2-a923-4cf7-b4dc-d0724de2e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51a88f-44f3-4bb8-8f2f-4869fa42c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/raw_text\n",
    "clusters = []\n",
    "with open(\"title_page_whole_test.txt\", \"r\") as fread:\n",
    "    for line in fread.readlines():\n",
    "        clusters.append(json.loads(line.strip()))\n",
    "%cd /Users/quert/Downloads/gcp_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b71fab-e22b-4bc3-ba17-46a9301d7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0].keys()\n",
    "# title_non_update_link\n",
    "# title_update_link\n",
    "# update_bs4\n",
    "# non_update_bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821ed47-256e-4b82-b46d-47e0fd79b941",
   "metadata": {},
   "source": [
    "### Get the indices of required instances\n",
    "* Keep English language, with non-updated full-content existed, and meaningful updated full-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b7f5f-256f-469c-851d-2ae8486682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idx list\n",
    "all_idx = [i for i in range(len(clusters))]\n",
    "rmved_idx = []\n",
    "\n",
    "for idx in range(len(clusters)):\n",
    "    try:\n",
    "        if bool(len(clusters[idx]['non_update_all_paragraph'].split('. ')) == 1):\n",
    "            rmved_idx.append(idx)\n",
    "    except:\n",
    "        error_idx.append(idx)\n",
    "idx_list = Diff(all_idx, rmved_idx)\n",
    "len(all_idx), len(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939a2d5-7574-4f8d-a5f2-5aae48ff246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmved_idx = []\n",
    "for idx in idx_list:\n",
    "    if len(clusters[idx]['update_first_paragraph'].split())<=10:\n",
    "        rmved_idx.append(idx)\n",
    "id_list = Diff(idx_list, rmved_idx)\n",
    "len(idx_list), len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3467a-a01a-4a7a-9c95-83c9749368fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by language of summ==English\n",
    "en_ids = []\n",
    "for idx in id_list:\n",
    "    if isEnglish(clusters[idx]['wiki_portal_summary']):\n",
    "        en_ids.append(idx)\n",
    "len(id_list), len(en_ids)\n",
    "# after filtering, we have 201 instances in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3badb8e-3790-4ecb-a074-799bd959e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92052637-4d52-4f0e-a18e-84a2c0b99d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id with same section name to dataframe\n",
    "# train_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./train_sec.csv')\n",
    "# test_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./test_sec.csv')\n",
    "# val_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./val_sec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdf0fa-1c5f-4812-aeab-0d0ddcc669e7",
   "metadata": {},
   "source": [
    "### Parse the unicode into text\n",
    "* Parse the unicode into text\n",
    "* Paragraphs are separate with `. \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f89c23-5525-4ccb-a954-ad2e0cdebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # bef_rec = []\n",
    "        non_update_all_paragraph = clusters[idx]['non_update_all_paragraph'].split('\\n')\n",
    "        # bef_rec.append(non_update_all_paragraph)\n",
    "        s = '.\\c'\n",
    "        bef_rec_str = s.join(non_update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', bef_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd382e9d-1043-438a-82b5-3ab5db34e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # tgt_rec = []\n",
    "        update_all_paragraph = clusters[idx][\"update_all_paragraph\"].split(\"\\n\")\n",
    "        # tgt_rec.extend(update_all_paragraph)\n",
    "        s = \".\\c\"\n",
    "        tgt_rec_str = s.join(update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', tgt_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e1b07-e3cc-4a6d-9f8b-9a483cd30ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text (src, tgt) to pt format\n",
    "srcs, tgts = [], []\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx].lstrip('{\"').lstrip(\"{'\").rstrip('\\n')\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180b606-e927-476d-9da0-7784458e0814",
   "metadata": {},
   "source": [
    "### Extract the section name and number of paragraphs under each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146defc-93f5-48f4-b83a-b789b1dcb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Old_pars_num(clu_idx):\n",
    "    nonupdated_raw_text = clusters[clu_idx][\"non_update_bs4\"]\n",
    "    # end_idx = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', raw_text).index(\"References\")\n",
    "    # sec_names = re.findall('<span class=\"toctext\">(.*)</span>', raw_text)[:end_idx]\n",
    "    \n",
    "    old_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', nonupdated_raw_text)\n",
    "    old_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', nonupdated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    old_main_global_ids = []\n",
    "    for old_main_sec in old_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = old_all_secs.index(old_main_sec)\n",
    "        old_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    # Some page only include summary without other sections\n",
    "    old_mod_secs = old_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(old_mod_secs)):\n",
    "            if idx in old_main_global_ids: old_mod_secs[idx]==old_mod_secs[idx]\n",
    "            else: \n",
    "                old_bool_lst = list(np.asarray(old_main_global_ids)<idx)\n",
    "                old_parent_ord = [str(val) for val in old_bool_lst].count(\"True\")\n",
    "                old_parent_idx = old_main_global_ids[old_parent_ord-1]\n",
    "                old_mod_secs[idx] = str(old_mod_secs[old_parent_idx]) + \" - \" + old_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        old_end_idx = old_mod_secs.index(\"References\")\n",
    "        old_sec_names = old_mod_secs[:old_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs for each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        old_num_pars = []\n",
    "        for i in range(len(old_sec_names)):\n",
    "            old_pars_str = str(nonupdated_raw_text.split('<span class=\"mw-headline')[i+1])\n",
    "            old_num_par = len(re.split(\"<p>|<li>\", old_pars_str)[1:])\n",
    "            # old_num_par = len(re.split(\"<p>\", old_pars_str)[1:])\n",
    "            old_num_pars.append(old_num_par)\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_num_pars.insert(0, old_num_summ)\n",
    "        old_sec_names.insert(0, \"Summary\")\n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        if \"Notes\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except: # page with only \"Summary\" exists\n",
    "        old_sec_names, old_num_pars = [], []\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_sec_names.append(\"Summary\")\n",
    "        old_num_pars.append(old_num_summ) \n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(old_secs_pars_nums)\n",
    "    return list(old_secs_pars_nums.keys())\n",
    "    # return old_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31b26c-2ae7-4afb-9ba8-3236b3768e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_pars_num(clu_idx):\n",
    "    updated_raw_text = clusters[clu_idx][\"update_bs4\"] \n",
    "    \n",
    "    new_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', updated_raw_text)\n",
    "    new_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', updated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    new_main_global_ids = []\n",
    "    for new_main_sec in new_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = new_all_secs.index(new_main_sec)\n",
    "        new_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    new_mod_secs = new_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(new_mod_secs)):\n",
    "            if idx in new_main_global_ids: new_mod_secs[idx]==new_mod_secs[idx]\n",
    "            else: \n",
    "                new_bool_lst = list(np.asarray(new_main_global_ids)<idx)\n",
    "                new_parent_ord = [str(val) for val in new_bool_lst].count(\"True\")\n",
    "                new_parent_idx = new_main_global_ids[new_parent_ord-1]\n",
    "                new_mod_secs[idx] = str(new_mod_secs[new_parent_idx]) + \" - \" + new_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        new_end_idx = new_mod_secs.index(\"References\")\n",
    "        new_sec_names = new_mod_secs[:new_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs under each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        new_num_pars = []\n",
    "        for i in range(len(new_sec_names)):\n",
    "            new_pars_str = str(clusters[clu_idx][\"update_bs4\"].split('<span class=\"mw-headline')[i+1])\n",
    "            new_num_par = len(re.split(\"<p>|<li>\", new_pars_str)[1:])\n",
    "            # new_num_par = len(re.split(\"<p>\", new_pars_str)[1:])\n",
    "            new_num_pars.append(new_num_par)\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_num_pars.insert(0, new_num_summ)\n",
    "        new_sec_names.insert(0, \"Summary\")\n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "        if \"Notes\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except:\n",
    "        new_sec_names, new_num_pars = [], []\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_sec_names.append(\"Summary\")\n",
    "        new_num_pars.append(new_num_summ) \n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(new_secs_pars_nums)\n",
    "    return list(new_secs_pars_nums.keys())\n",
    "    # return new_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b08f8a-aa6b-4adb-a7d0-e41cc7916981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the instances with same sections\n",
    "sec_ids = []\n",
    "for idx in en_ids:\n",
    "    try:\n",
    "        if Old_pars_num(idx)==New_pars_num(idx):\n",
    "            sec_ids.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "# -- Train set\n",
    "# We have 911 instances with same #sections and #paragraphs\n",
    "# We have 1334 instances with same #sections -> 423 instances with same #sections and different #paragraphs\n",
    "# Apply the sec_ids to extract and construct new datset\n",
    "# -- Test set\n",
    "# We have 168 instances with same #sections and #paragraphs\n",
    "# We have 168 instances with same #sections -> 0 instances with same #sections and different #paragraphs\n",
    "# -- Val set\n",
    "# We have instances with same #sections and #paragraphs\n",
    "# We have instances with same #sections -> instances with same #sections and different #paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804632a6-7aa9-497c-b0e8-757eb61bbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec692dea-fda5-443c-be61-4669faca3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our unsized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_old/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81af69-1b01-43a4-b186-5c765a78bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the paragraphs\n",
    "new_srcs, new_tgts = [], []\n",
    "for idx in range(len(srcs)):\n",
    "    pars = srcs[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(Old_pars_num(sec_ids[idx]).values()))\n",
    "    new_src = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_srcs.append(new_src)\n",
    "for idx in range(len(tgts)):\n",
    "    pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(New_pars_num(sec_ids[idx]).values()))\n",
    "    new_tgt = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_tgts.append(new_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8fe7a-95ac-4ef7-bf48-11b3a3a183e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create re-sized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "with open(\"train_text.txt.src\", \"w\") as f:\n",
    "    for new_src in new_srcs:\n",
    "        f.write(new_src+\"\\n\")\n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for new_tgt in new_tgts:\n",
    "        f.write(new_tgt+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849d86b-2b78-4a51-aebc-77471fb56af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert src, tgt to pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "wrap = []\n",
    "for idx in range(len(new_srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = new_srcs[idx]\n",
    "    idx_content['summary'] = new_tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff562b-3803-4423-9596-014d2496ca87",
   "metadata": {},
   "source": [
    "### Insert section names to each paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289ba9c-d82b-4e63-b631-e096e7d27217",
   "metadata": {},
   "source": [
    "* Check if the #paragraphs from instances equals to our counts\n",
    "* `. \\\\c\\\\c` -> `. <Timeline> \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711eef6c-7b86-4ba2-b200-97145816df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the numbers of each section name (dicts) into (list) \n",
    "old_list_secnums = [Old_pars_num(idx) for idx in sec_ids]\n",
    "new_list_secnums = [New_pars_num(idx) for idx in sec_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c92594-9d71-4e0d-922c-2792e79fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the labeled data\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_new/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf269d33-fc4f-4ee9-b2fc-598d6209ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(list(old_list_secnums[0].values())), len(srcs[0].split(\"\\\\c\\\\c\"))\n",
    "# new_srcs = []\n",
    "'''\n",
    "nums = list(old_list_secnums[0].values())\n",
    "titles = list(old_list_secnums[0].keys())\n",
    "src_article = srcs[0]\n",
    "paragraphs = src_article.split(\"\\\\c\\\\c\")\n",
    "paragraphs[0] = paragraphs[0]+title[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04521fa-5ee9-4716-8c5e-443865e13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts into indices: (index: title)\n",
    "titles_keys = []\n",
    "for idx in range(len(new_list_secnums)):\n",
    "    titles = list(old_list_secnums[idx].keys())\n",
    "    titles_as_keys = []\n",
    "    for i in range(len(titles)):\n",
    "        for _ in range(list(new_list_secnums[idx].values())[i]):\n",
    "            titles_as_keys.append(titles[i])\n",
    "    titles_keys.append(titles_as_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e0640-94e2-491f-a48a-7f1db2410b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add section names to tail for each paragraph\n",
    "added_secs = []\n",
    "for idx in range(len(titles_keys)):\n",
    "    splitted_pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    pars_in = []\n",
    "    for i in range(len(splitted_pars)):\n",
    "        par_with_secname = splitted_pars[i] + \" <\" + titles_keys[idx][i] + \">\" + \" \\\\c\\\\c\"\n",
    "        par_with_secname = par_with_secname.replace(\"  <\", \" <\")\n",
    "        pars_in.append(par_with_secname)\n",
    "    pars = \"\".join(pars_in)\n",
    "    added_secs.append(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83710fb1-2b89-438f-95fb-f18567ec392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our new-construct data into src, tgt\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/ \n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for instance in added_secs:\n",
    "        f.write(instance+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81f34b-2559-4b0b-88c6-52c6e052469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert src, tgt into single pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "srcs, tgts = [], []\n",
    "with open(\"train_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"train_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fe501-95dd-4a62-b3cd-4b012ede279d",
   "metadata": {},
   "source": [
    "### Trigger Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fd7fc-08dd-4d3a-bdc1-223f7e623309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "triggers = [clusters[idx][\"wiki_portal_summary\"] for idx in sec_ids]\n",
    "with open(\"samesecs_triggers_test.txt\", \"w\") as f:\n",
    "    for trigger in triggers:\n",
    "        f.write(trigger+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3ae5a-b764-48de-9361-889996fe56b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract paragraphs from each instance, and label as edited 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d255d-95b9-4811-a539-41ff4e6b398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled/\n",
    "val_pt = torch.load(\"val_samesecs_labeled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858ec35-b5fe-4a3c-887b-f6824d67f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maen modification from each instance, and labeled 1 if bigger than mean value, else 1.\n",
    "# Concate the paragraphs with its respective trigger\n",
    "# Calculate the mean editions for each instance\n",
    "mean_editions = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        editions = []\n",
    "        num_editions = len(re.findall(r\"\\[ADD]\", par)) + len(re.findall(r\"\\[SUB]\", par)) + len(re.findall(r\"\\[RM]\", par))\n",
    "        editions.append(num_editions)\n",
    "    mean_editions.append(np.mean(editions))\n",
    "\n",
    "labels = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        if (len(re.findall(r\"\\[ADD]\", par)) + len(re.findall(r\"\\[SUB]\", par)) + len(re.findall(r\"\\[RM]\", par))) > np.mean(mean_editions[idx]): labels.append(1)\n",
    "        else: labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05b695-b1bf-4892-85f7-f770c2b56d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col[0]: paragraph + section name\n",
    "col_0 = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "    # for par in val_pt[idx][\"document\"].split(\"\\\\c\\\\c\"):\n",
    "        col_0.append(par)\n",
    "    \n",
    "# col[1]: trigger\n",
    "triggers, col_1 = [], []\n",
    "with open(\"samesecs_triggers_val.txt\", \"r\") as f:\n",
    "    for trigger in f.readlines():\n",
    "        triggers.append(trigger)\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        col_1.append(triggers[idx])\n",
    "\n",
    "# col[2]: target (label)\n",
    "col_2 = labels.copy()\n",
    "assert len(col_0)==len(col_1)==len(col_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e37ee5-047d-4ba8-aee4-32c47ab74bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the sentences with [ADD] at head\n",
    "old_par = col_0.copy()\n",
    "instances_par = []\n",
    "for idx in range(len(old_par)):\n",
    "    splits = re.split(r\"\\[KEEP\\]\", old_par[idx])\n",
    "    splits = [ele for ele in splits if ele]\n",
    "    for split in splits:\n",
    "        try:\n",
    "            if split.split()[0] in [\"[ADD]\", \"[SUB]\"]: splits.remove(split)\n",
    "        except:\n",
    "            pass\n",
    "    instance = (\"\".join(splits)).replace(\"  \", \" \")\n",
    "    instances_par.append(instance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86b62f-bdcd-4416-8fd6-7092d6491964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cols into dataframe\n",
    "assert len(instances_par)==len(col_1)==len(col_2)\n",
    "df = pd.DataFrame()\n",
    "df[\"paragraphs_secs\"] = instances_par\n",
    "df[\"trigger\"] = col_1\n",
    "df[\"target\"] = col_2\n",
    "df.to_csv(\"merged_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65df142-c7a1-4796-beba-3d27df844723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the paragraphs and tirgger columns into sources\n",
    "df = pd.read_csv(\"merged_val.csv\")\n",
    "concats = []\n",
    "for idx in range(df.shape[0]):\n",
    "    row = df.iloc[idx, :]\n",
    "    concated = str(row[\"paragraphs_secs\"]) + \" -- \" + str(row[\"trigger\"])\n",
    "    concats.append(concated)\n",
    "assert len(concats)==df[\"target\"].shape[0]\n",
    "df_updated = pd.DataFrame()\n",
    "df_updated[\"paragraph\"] = concats\n",
    "df_updated[\"target\"] = df[\"target\"].tolist()\n",
    "df_updated.to_csv(\"merged_updated_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e8d75-0e6e-4e05-98d5-bfc5af0625df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"merged_updated_val.csv\")\n",
    "df_train.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae1652-ce89-4f57-aff3-8ae6c27336d9",
   "metadata": {},
   "source": [
    "### Match paragraphs to respective instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54b6cc-55ef-49c6-93dc-f941aa48d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled/\n",
    "test_pt = torch.load(\"test_samesecs_labeled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5125a72-edd7-4a58-90ca-8d03a9cd3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_par = []\n",
    "for idx in range(len(test_pt)):\n",
    "    num_par.append(len(test_pt[idx][\"summary\"].split(\"\\\\c\\\\c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5ddad-05d7-42fe-92ff-5b8a82dfc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"Num of pars\": num_par}).to_csv(\"./merged_numpar_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bbcb1-1fb6-475d-afec-ff50a8a68eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_updated_test.csv\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b0bf9-586b-4678-b350-c64051a7a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df.iloc[:, 1]\n",
    "df_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e082999-62e6-4503-9f2f-865b0e5c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = df_slice.to_list()\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e51f6f-df9f-4c3b-ba43-78d524fd606c",
   "metadata": {},
   "source": [
    "#### Match paragraphs to its respective instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e93adf-c723-468f-8313-47bc28d237a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(\"submission.csv\")\n",
    "test_csv = pd.read_csv(\"merged_updated_test.csv\")\n",
    "num_par = pd.read_csv(\"merged_numpar_test.csv\")\n",
    "par_nums = num_par.iloc[:, 1].to_list()\n",
    "assert len(sub) == np.sum(num_par.iloc[:, 1].to_list()) == len(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d2399-e505-4359-9ddd-2ec1bb3d2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge paragraphs to instances\n",
    "instances = []\n",
    "start = 0\n",
    "# for num in par_nums:\n",
    "for idx in range(len(par_nums)):\n",
    "    end_idx = start_idx + par_nums[idx] - 1\n",
    "    the_range = [start_idx, end_idx]\n",
    "    start = end_idx + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7ef9a-aebd-455a-8a10-d51b45e7552f",
   "metadata": {},
   "source": [
    "### Fix the unbalanced problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4c913-1833-4d88-841f-957feb040c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_updated_train.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad46178-5d48-4402-9e90-0cabb0e55bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.iloc[:, 1].to_list()\n",
    "true_idx = [i for i in range(len(target)) if target[i]==1]\n",
    "false_idx = [i for i in range(len(target)) if target[i]==0]\n",
    "len(true_idx), len(false_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f826f-8d3f-43c9-bb80-aa1c09602569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample 3 times of false instances\n",
    "from random import sample\n",
    "resample_false_idx = sorted(sample(false_idx, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408a84f-d937-480a-ac76-8732ac7638f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the false example\n",
    "resample_par, resample_idx = [], []\n",
    "for idx in resample_false_idx:\n",
    "    resample_par.append(df.iloc[idx, 0])\n",
    "    resample_idx.append(df.iloc[idx, 1])\n",
    "# combline the false and true example\n",
    "for idx in true_idx:\n",
    "    resample_par.append(df.iloc[idx, 0])\n",
    "    resample_idx.append(df.iloc[idx, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c50226-de6f-4291-b8bd-d7b35189b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"paragraph\": resample_par, \"target\": resample_idx}).to_csv(\"resample_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83a64f-4288-4b92-ad31-ee879ab35f24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examine the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8dd8a-7e11-4222-b915-d2c09bb58b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(\"submission.csv\")\n",
    "df_submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dc1cf-2202-4b3c-b928-8318c429a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026fcbc-00fa-4522-92c9-0c83fc502f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the test data targets\n",
    "df_test = pd.read_csv(\"merged_updated_test.csv\")\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ec77c-d644-40eb-a280-d3957ae150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e776f-53f2-4711-a420-e62691667b94",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9183fd-ee8c-4d95-ad7b-c5682a7c71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./ptfile/same_secs_insert_labeled\n",
    "# df_test = pd.read_csv(\"ptfile/same_secs_insert_labeled/merged_test.csv\")\n",
    "# df_test.info()\n",
    "test_pt = torch.load(\"test_samesecs_labeled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301ed55-f0cd-4f96-adcb-dcb5f0813eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pt to src and tgt to recalculate the baseline\n",
    "with open(\"original_test.src\", \"w\") as f:\n",
    "    for idx in range(len(test_pt)):\n",
    "        line = test_pt[idx][\"document\"].strip().replace(\"\\n\", \"\\c\")\n",
    "        f.write(line+\"\\n\")\n",
    "with open(\"original_test.tgt\", \"w\") as f:\n",
    "    for idx in range(len(test_pt)):\n",
    "        line = test_pt[idx][\"summary\"].strip().replace(\"\\n\", \"\\c\")\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5429a-1622-467b-80a8-20c8240e7005",
   "metadata": {},
   "source": [
    "### Construct data for decoder settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404ce8f6-1ac8-4aea-ba73-baae0f4da83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './ptfile/same_secs_insert_labeled'\n",
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled\n"
     ]
    }
   ],
   "source": [
    "%cd ./ptfile/same_secs_insert_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf5c4de-d117-40d6-a6e3-614fe9d105ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classification report\n",
    "class_rep = pd.read_csv(\"./submission.csv\")\n",
    "# load the respective paragraphs (paragraphs + section name + trigger)\n",
    "test_pars = pd.read_csv(\"./merged_updated_test.csv\")\n",
    "\n",
    "assert len(class_rep)==len(test_pars)\n",
    "# 10425 paragraphs (in the test data)\n",
    "# we only need to extract the positive paragraphs from test data, update them, then merging -> 168 instances\n",
    "# extract hthe 87 instance, do the eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f34874-5fe2-4764-a5f5-bb8e4dca6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the paragraphs needed to be updated, and its triggers\n",
    "# class_rep.target.value_counts()\n",
    "paragraphs_ids = [idx for idx in range(len(class_rep)) if class_rep.iloc[idx, 0]==1]\n",
    "# we have 371 paragraphs to update\n",
    "# extract triggers for 371 paragraphs\n",
    "triggers = [test_pars.iloc[idx, 0].split(\". \")[-2] for idx in paragraphs_ids]\n",
    "paragraphs = [\". \".join(test_pars.iloc[1, 0].split(\". \")[0:-2]) for idx in paragraphs_ids]\n",
    "assert len(paragraphs_ids)==len(triggers)==len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6ff454-437c-47e2-bed7-c41ef11772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the prompts\n",
    "prompts = []\n",
    "for idx in range(len(triggers)):\n",
    "    prompt = f\"\"\"\n",
    "As an article writer, you have to return the UPDATED CONTENT according to given OLD CONTENT and a TRIGGERED NEWS.\\n\n",
    "OLD CONTENT:\\n\n",
    "{paragraphs[idx]}\n",
    "\\nTRIGGERED NEWS:\\n\n",
    "{triggers[idx]}\n",
    "\"\"\"\n",
    "    prompts.append(prompt)\n",
    "# now, we have 371 prompts (prompts + paragraphs + section names + triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8e476ca-8b1f-404d-b42a-b5b3a757dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the combination to csv file\n",
    "pd.DataFrame({\"prompt\": prompts}).to_csv(\"./for_decoder_exp/prompts_paragraphs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb7f4ccd-e749-414e-99a0-87367f9d07f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs an article writer, you have to return the UPDATED CONTENT according to given OLD CONTENT and a TRIGGERED NEWS.\\n\\nOLD CONTENT:\\n\\nDuring the Afghan Civil War (1996–2001), resistance to the Taliban was strongest in northern Afghanistan, the base of the Northern Alliance. According to the Afghanistan Analysts Network, the Taliban\\'s concentration of its forces in the north may be an attempt to forestall the creation of a second Northern Alliance after the withdrawal of U.S. forces. <Background> -- The Taliban captures Zaranj, the provincial capital of Nimruz Province, making it the first major capture of a provincial city by the group since the 2001 invasion\\n\\nTRIGGERED NEWS:\\n\\nPresident Umaro Sissoco Embalo says that \"many\" members of the security forces have been killed in a \"failed attack against democracy\" as both the African Union and ECOWAS condemn the attempted coup\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c328c-ad86-4516-becb-fbc57888c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the prompts to LLM\n",
    "# 1. GPT-3.5\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "responses = []\n",
    "count = 0\n",
    "for idx in range(len(prompts)):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompts[idx]}\n",
    "        ]\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    responses.append(response)\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        time.sleep(300)\n",
    "\n",
    "\n",
    "# 2. GPT-4\n",
    "# 3. LLaMA (7B)\n",
    "# 4. Vicuna (13B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "59330baa-6eb1-41cc-829e-07724489c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lengths statistics for prompts and ground truths (paragraph-level)\n",
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "lengths = []\n",
    "for idx in range(len(prompts)):\n",
    "    length = len(tokenizer(prompts[idx])[\"input_ids\"])\n",
    "    lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50130e7-a100-4901-a961-c86604718992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21ed8e03-11b3-4c47-be18-4be9b36a0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune Vicuna: with paragraph-level data\n",
    "# check the labels from merged_update_x.csv\n",
    "check_label_csv = pd.read_csv(\"./merged_updated_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "793fc26e-6ad5-4ff8-a35a-a21d2d0a5edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71874\n",
       "1     1972\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_label_csv.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322cb026-3fa9-484d-b6fe-05c79fdd1dc6",
   "metadata": {},
   "source": [
    "### Prepare data for finetuning Vicuna (paragraph-level) as decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4233f7d3-174f-4ad9-89ba-90977408c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the preprocessed json data\n",
    "# load train data (paragraph-level)\n",
    "train_data = pd.read_csv(\"ptfile/same_secs_insert_labeled/merged_train.csv\")\n",
    "pos_ids = [idx for idx in range(len(train_data)) if train_data.iloc[idx, 2]==1] \n",
    "neg_ids = [idx for idx in range(len(train_data)) if idx not in pos_ids]\n",
    "neg_ids = sorted(random.sample(neg_ids, len(pos_ids)))\n",
    "assert len(pos_ids)==len(neg_ids)\n",
    "# extracted triggers for pos and neg in order (pos -> neg)\n",
    "# sort the merged ids for pos + neg ids\n",
    "ids = sorted(pos_ids + neg_ids)\n",
    "# extracted the pos and neg samples\n",
    "# merge pos and neg samples to dataset\n",
    "triggers = [train_data.iloc[idx, 1].strip() for idx in ids]\n",
    "paragraphs_secs = [train_data.iloc[idx, 0] for idx in ids]\n",
    "assert len(ids)==len(triggers)==len(paragraphs_secs)\n",
    "\n",
    "# extract the references from article to paragraph-level according to the num of pars\n",
    "nums_df = pd.read_csv(\"./ptfile/same_secs_insert_labeled/merged_numpar_train.csv\")\n",
    "nums_df.rename(columns={'Num of pars': 'nums'}, inplace=True)\n",
    "nums = list(nums_df.nums.values) # len(nums)== #articles\n",
    "references, full_refs, sel_references = [], [], []\n",
    "with open(\"./ptfile/same_secs_insert/train_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        full_refs.append(line)\n",
    "for idx in range(len(nums)):\n",
    "    for n in range(int(nums[idx])):\n",
    "        references.append(full_refs[idx])\n",
    "assert len(references)==np.sum(nums)\n",
    "sel_references = [references[idx] for idx in ids]\n",
    "assert len(sel_references)==len(ids)\n",
    "\n",
    "# variables: paragraphs with section: paragraphs_secs; sorted global ids of selected pos and neg: ids; triggers according to ids: triggers; sel_references: extracted references according to ids in paragraph-level;\n",
    "# full_refs: references of each article; references: references according to each paragraph;\n",
    "# combine data with prompts\n",
    "dataset_data = [\n",
    "    {\n",
    "        \"instruction\": \"As an article writer, you have to return the UPDATED CONTENT according to given OLD CONTENT and a TRIGGERED NEWS.\\n\",\n",
    "        \"input\": f\"\"\"OLD CONTENT: {paragraphs_secs[idx]} TRIGGERED NEWS: {triggers[idx]} \"\"\",\n",
    "        \"output\": sel_references[idx]\n",
    "    }\n",
    "    for idx in range(len(ids))\n",
    "]\n",
    "\n",
    "\n",
    "# Dump our data to json format\n",
    "with open(\"./ptfile/vicuna_lora/merge.json\", \"w\") as f:\n",
    "    json.dump(dataset_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aeb537e-8ba2-4b63-af00-f96ad4b79872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restruncate the outputs from decoder mode to respective instances\n",
    "updated_pars = []\n",
    "with open(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/outputs.hyp\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        updated_pars.append(line)\n",
    "assert len(updated_pars)==371\n",
    "\n",
    "test_csv = pd.read_csv(\"./ptfile/same_secs_insert_labeled/merged_updated_test.csv\") # \n",
    "sub_csv = pd.read_csv(\"./ptfile/same_secs_insert_labeled/submission.csv\")\n",
    "nums_df = pd.read_csv(\"./ptfile/same_secs_insert_labeled/merged_numpar_test.csv\")\n",
    "instances = test_csv.paragraph.values\n",
    "targets = sub_csv.target.values\n",
    "pos_ids = [idx for idx in range(len(targets)) if targets[idx]==1]\n",
    "\n",
    "\n",
    "merged_instances = instances.copy()\n",
    "for idx in range(len(pos_ids)):\n",
    "    merged_instances[pos_ids[idx]] = updated_pars[idx]\n",
    "    \n",
    "pd.DataFrame({\"paragraph\": merged_instances}).to_csv(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/updated_pars.csv\")\n",
    "updated_df = pd.read_csv(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/updated_pars.csv\")\n",
    "\n",
    "nums_df.rename(columns={'Num of pars': 'nums'}, inplace=True)\n",
    "nums = list(nums_df.nums.values)\n",
    "\n",
    "# main algo\n",
    "hyp_instances = []\n",
    "start = 0\n",
    "for num in nums:\n",
    "    end = start + num - 1\n",
    "    # sel_range = [start, end]\n",
    "    txt_in_range = updated_df.iloc[start:end+1, 1].to_list()\n",
    "    instance = \". \".join(txt_in_range)\n",
    "    hyp_instances.append(instance)\n",
    "    start += num\n",
    "    \n",
    "ref_instances = []\n",
    "with open(\"./ptfile/same_secs_insert/test_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        ref_instances.append(line.strip())\n",
    "assert len(hyp_instances)==len(ref_instances)\n",
    "\n",
    "pd.DataFrame({\"hyp\": hyp_instances, \"ref\": ref_instances}).to_csv(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/final_merged_vicuna_vanilla.csv\")\n",
    "\n",
    "# decompress the final_merged to hyp and ref separately for calculating rouge\n",
    "final_merged_csv = pd.read_csv(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/final_merged_vicuna_vanilla.csv\")\n",
    "hyps = final_merged_csv.hyp.to_list()\n",
    "refs = final_merged_csv.ref.to_list()\n",
    "\n",
    "with open(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/final_hyp.hyp\", \"w\") as f:\n",
    "    for hyp in hyps:\n",
    "        f.write(hyp.replace(\"\\n\", \"\\c\")+\"\\n\")\n",
    "with open(\"./ptfile/same_secs_insert_labeled/vicuna_vanilla/final_ref.ref\", \"w\") as f:\n",
    "    for ref in refs:\n",
    "        f.write(ref.replace(\"\\n\", \"\\c\")+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e8637-ffab-4ab5-8ba7-0f86b0662032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 87 instances from ground truth\n",
    "# find the 87 instances from 168 instances\n",
    "short_csv = pd.read(\"./ptfile/same_secs_insert_labeled/chatgpt/chatgpt_full.csv\")\n",
    "short_global_ids = short_csv.idx.to_list() # 87\n",
    "# find the 168 global ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46fcbb-6259-453a-b96a-46dcb90a6a09",
   "metadata": {},
   "source": [
    "### Check the outputs from finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36311a4a-9987-43fe-a15e-82c25a5e94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restruncate the outputs from decoder mode to respective instances\n",
    "updated_pars = []\n",
    "output_csv = pd.read_csv(\"/Users/quert/Downloads/gcp_tmp/ptfile/vanilla_llama_alpaca/llama-alpaca_outputs.csv\")\n",
    "updated_pars = [output_csv.iloc[idx, 1].replace(\"\\n\", \"\\c\") for idx in range(len(output_csv))]\n",
    "assert len(updated_pars)==371\n",
    "\n",
    "test_csv = pd.read_csv(\"./ptfile/same_secs_insert_labeled/merged_updated_test.csv\") # \n",
    "sub_csv = pd.read_csv(\"./ptfile/same_secs_insert_labeled/submission.csv\")\n",
    "nums_df = pd.read_csv(\"./ptfile/same_secs_insert_labeled/merged_numpar_test.csv\")\n",
    "instances = test_csv.paragraph.values\n",
    "targets = sub_csv.target.values\n",
    "pos_ids = [idx for idx in range(len(targets)) if targets[idx]==1]\n",
    "\n",
    "\n",
    "merged_instances = instances.copy()\n",
    "for idx in range(len(pos_ids)):\n",
    "    merged_instances[pos_ids[idx]] = updated_pars[idx]\n",
    "    \n",
    "pd.DataFrame({\"paragraph\": merged_instances}).to_csv(\"./ptfile/same_secs_insert_labeled/vicuna_finetuned/updated_pars.csv\")\n",
    "updated_df = pd.read_csv(\"./ptfile/same_secs_insert_labeled/vicuna_finetuned/updated_pars.csv\")\n",
    "\n",
    "nums_df.rename(columns={'Num of pars': 'nums'}, inplace=True)\n",
    "nums = list(nums_df.nums.values)\n",
    "\n",
    "# main algo\n",
    "hyp_instances = []\n",
    "start = 0\n",
    "for num in nums:\n",
    "    end = start + num - 1\n",
    "    # sel_range = [start, end]\n",
    "    txt_in_range = updated_df.iloc[start:end+1, 1].to_list()\n",
    "    instance = \". \".join(txt_in_range)\n",
    "    hyp_instances.append(instance)\n",
    "    start += num\n",
    "    \n",
    "ref_instances = []\n",
    "with open(\"./ptfile/same_secs_insert/test_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        ref_instances.append(line.strip())\n",
    "assert len(hyp_instances)==len(ref_instances)\n",
    "\n",
    "pd.DataFrame({\"hyp\": hyp_instances, \"ref\": ref_instances}).to_csv(\"/Users/quert/Downloads/gcp_tmp/ptfile/vanilla_llama_alpaca/final_merged.csv\")\n",
    "\n",
    "# decompress the final_merged to hyp and ref separately for calculating rouge\n",
    "final_merged_csv = pd.read_csv(\"/Users/quert/Downloads/gcp_tmp/ptfile/vanilla_llama_alpaca/final_merged.csv\")\n",
    "hyps = final_merged_csv.hyp.to_list()\n",
    "refs = final_merged_csv.ref.to_list()\n",
    "\n",
    "with open(\"/Users/quert/Downloads/gcp_tmp/ptfile/vanilla_llama_alpaca/final_hyp.hyp\", \"w\") as f:\n",
    "    for hyp in hyps:\n",
    "        f.write(hyp.replace(\"\\n\", \"\\c\")+\"\\n\")\n",
    "with open(\"/Users/quert/Downloads/gcp_tmp/ptfile/vanilla_llama_alpaca/final_ref.ref\", \"w\") as f:\n",
    "    for ref in refs:\n",
    "        f.write(ref.replace(\"\\n\", \"\\c\")+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fc348c6-4b17-4dc3-b853-db355364cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/edit_NetKu/generation/finetuned-vicuna_13b_as_decoder\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/edit_NetKu/generation/finetuned-vicuna_13b_as_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7770f165-b65e-426e-b2c7-fe0ba26cbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv(\"final_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "182ec13d-dce5-4599-93b3-21cae18d40f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168 entries, 0 to 167\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  168 non-null    int64 \n",
      " 1   hyp         168 non-null    object\n",
      " 2   ref         168 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "csv_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb62fa4e-e331-4f0f-ae0a-fb5eece8ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = csv_file.ref.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7871b42c-1dbc-4bcb-b104-23bb2d4b26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3779 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lengths statistics for prompts and ground truths (paragraph-level)\n",
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "lengths = []\n",
    "for idx in range(len(hyps)):\n",
    "    length = len(tokenizer(hyps[idx])[\"input_ids\"])\n",
    "    lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce1769ed-b6e6-4c99-ab6d-7a4e567ddaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 67597, 5766.785714285715, 3167.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(lengths),np.max(lengths),np.mean(lengths), np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c277ded-9567-44e5-9425-d5743af04d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73846, 10425, 11253)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"/Users/quert/Documents/GitHub/edit_NetKu/dataset/same_secs_insert_labeled/merged_train.csv\")\n",
    "test_csv = pd.read_csv(\"/Users/quert/Documents/GitHub/edit_NetKu/dataset/same_secs_insert_labeled/merged_test.csv\")\n",
    "val_csv = pd.read_csv(\"/Users/quert/Documents/GitHub/edit_NetKu/dataset/same_secs_insert_labeled/merged_val.csv\")\n",
    "\n",
    "len(train_csv), len(test_csv), len(val_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
